{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options Trading Tool: Machine Learning Exercises\n",
    "* **Notebook 1: Data Aggregation** – Collects & preprocesses stock & options data  \n",
    "* **Notebook 2: Hypothesis Testing** – Tests profitability patterns & market behaviors  \n",
    "* **Notebook 3: Statistical Analysis** – Summarizes results & evaluates statistical significance  \n",
    "* **Notebook 4: Summary & Insights** – Identifies actionable trading patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Aggregation\n",
    "* Library Imports\n",
    "* Parameters\n",
    "* Helper Functions\n",
    "* Downloading Underlying Data\n",
    "* Determining Option Contracts\n",
    "* Downloading Option Data\n",
    "* Reading Data from CSVs\n",
    "* Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import talib as ta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API and Date Parameters\n",
    "api_key = 'Enter API KEY'\n",
    "api_secret = 'Enter API Secret'\n",
    "\n",
    "# Today's date (really prior date to avoid conflicts)\n",
    "today_date = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Symbol and Date Range\n",
    "symbol = 'SPY'\n",
    "start_date = \"2024-01-01\"\n",
    "# end_date = today_date\n",
    "end_date = \"2024-12-31\"\n",
    "max_calls_per_minute = 150  # For rate limiting purposes\n",
    "base_folder = 'Historical OHCL Bars'  # Folder to save data\n",
    "strike_threshold = 0  # Strikes around underlying price to consider\n",
    "days_offset = 2  # Number of additional days in the future to consider for options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to read CSVs from a folder\n",
    "def read_all_csvs(base_folder='Historical OHCL Bars'):\n",
    "    paths = {\n",
    "        'options_full_day': os.path.join(base_folder, 'Options'),\n",
    "        'underlying': os.path.join(base_folder, 'Underlying')\n",
    "    }\n",
    "\n",
    "    def read_and_concat_files(folder):\n",
    "        if os.path.exists(folder):\n",
    "            files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "            if files:\n",
    "                dfs = [pd.read_csv(f) for f in files if os.stat(f).st_size > 0]\n",
    "                if dfs:\n",
    "                    return pd.concat(dfs, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = {\n",
    "        'options_full_day': read_and_concat_files(paths['options_full_day']),\n",
    "        'underlying': read_and_concat_files(paths['underlying'])\n",
    "    }\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to save CSV data\n",
    "def save_to_csv(data, date_str, session, folder, suffix):\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    file_path = os.path.join(folder, f'{suffix}_{date_str}.csv')\n",
    "    data.to_csv(file_path, index=False)\n",
    "    print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to generate option symbol\n",
    "def generate_option_symbol(symbol, exp_date, call_put, strike_price_dollar, strike_price_decimal):\n",
    "    strike_price = f\"{int(strike_price_dollar):05d}{int(strike_price_decimal):03d}\"\n",
    "    exp_date_formatted = datetime.strptime(exp_date, \"%Y-%m-%d\").strftime(\"%y%m%d\")\n",
    "    full_symbol = f\"{symbol}{exp_date_formatted}{call_put}{strike_price}\"\n",
    "    return full_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_strike_ranges(df, strike_threshold):\n",
    "    df['t'] = pd.to_datetime(df['timestamp'])  # Convert timestamp to datetime\n",
    "    df['date'] = df['t'].dt.date\n",
    "    # Use 'high' and 'low' based on your CSV structure\n",
    "    daily_ranges = df.groupby('date').agg({'low': 'min', 'high': 'max'})\n",
    "\n",
    "    strike_ranges = {}\n",
    "    for date, row in daily_ranges.iterrows():\n",
    "        low_end = row['low'] // 1\n",
    "        high_end = row['high'] // 1\n",
    "        low_strike = int(low_end) - strike_threshold\n",
    "        high_strike = int(high_end) + strike_threshold\n",
    "        strike_ranges[date] = (low_strike, high_strike)\n",
    "    \n",
    "    return strike_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_contracts_to_consider(df, low_strike, high_strike, expiration_dates):\n",
    "    \"\"\"\n",
    "    Generate unique contracts to consider for a specific day.\n",
    "    Contracts will be unique within a day but may repeat across different days.\n",
    "    \"\"\"\n",
    "    contracts = set()  # Use a set to ensure uniqueness within the day\n",
    "\n",
    "    # Iterate through the underlying data for the day\n",
    "    for _, row in df.iterrows():\n",
    "        for strike in range(int(low_strike), int(high_strike) + 1):\n",
    "            strike_price_dollar = int(strike)\n",
    "            strike_price_decimal = 0\n",
    "            for exp_date in expiration_dates:\n",
    "                call_contract = generate_option_symbol(row['symbol'], exp_date, 'C', strike_price_dollar, strike_price_decimal)\n",
    "                put_contract = generate_option_symbol(row['symbol'], exp_date, 'P', strike_price_dollar, strike_price_decimal)\n",
    "                \n",
    "                # Add contracts to the set (automatically handles uniqueness)\n",
    "                contracts.add(call_contract)\n",
    "                contracts.add(put_contract)\n",
    "\n",
    "    return list(contracts)  # Convert the set back to a list to return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to get expiration dates\n",
    "def get_expiration_dates(date_str, include_today=True, offset_days=2):\n",
    "    base_date = np.datetime64(datetime.strptime(date_str, \"%Y-%m-%d\"), 'D')\n",
    "    holidays = ['2024-01-01', '2024-07-04', '2024-12-25']\n",
    "    holidays_np = np.array(holidays, dtype='datetime64[D]')\n",
    "    start_day_offset = 0 if include_today else 1\n",
    "    expiration_dates = [\n",
    "        np.busday_offset(base_date, i, roll='forward', holidays=holidays_np).astype(str)\n",
    "        for i in range(start_day_offset, start_day_offset + offset_days)\n",
    "    ]\n",
    "    return [datetime.strptime(date, '%Y-%m-%d').strftime('%Y-%m-%d') for date in expiration_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_options_ohlc_to_csv(df, date_str, base_folder='Historical OHCL Bars'):\n",
    "    \"\"\"\n",
    "    Saves the consolidated options OHLC data to a CSV file for a given date.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The consolidated options data.\n",
    "        date_str (str): The date for which data is being saved.\n",
    "        base_folder (str): The folder where the file will be saved.\n",
    "    \"\"\"\n",
    "    # Define the file path for saving the data\n",
    "    options_folder = os.path.join(base_folder, 'Options')\n",
    "    os.makedirs(options_folder, exist_ok=True)\n",
    "    file_path = os.path.join(options_folder, f'options_{date_str}.csv')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the save_underlying_ohlc_to_csv function to clean up the DataFrame\n",
    "def save_underlying_ohlc_to_csv(stock_bars_data, date_str, symbol, base_folder='Historical OHCL Bars'):\n",
    "    \"\"\"Save underlying stock bars data to a CSV file.\"\"\"\n",
    "    underlying_folder = os.path.join(base_folder, 'Underlying')\n",
    "    os.makedirs(underlying_folder, exist_ok=True)\n",
    "\n",
    "    # Extract bars data from the response\n",
    "    bars = stock_bars_data['bars']\n",
    "\n",
    "    # Convert the bars data to a DataFrame\n",
    "    df = pd.DataFrame(bars)\n",
    "\n",
    "    # Add the symbol to the DataFrame\n",
    "    df['symbol'] = symbol\n",
    "\n",
    "    # Clean up the column names to be more intuitive\n",
    "    df.rename(columns={'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume', 't': 'timestamp', 'vw': 'vwap'}, inplace=True)\n",
    "\n",
    "    # Convert timestamp to a readable datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    # Define file path with date stamp\n",
    "    file_path = os.path.join(underlying_folder, f'underlying_{symbol}_{date_str}.csv')\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Underlying data for {symbol} on {date_str} saved successfully to {file_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Underlying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to fetch stock bars (OHLC) from Alpaca API\n",
    "def get_stock_bars(symbol, timeframe, start_date, end_date, api_key, api_secret, limit=10000, adjustment='raw', feed='sip', sort='asc'):\n",
    "    \"\"\"\n",
    "    Fetch OHLC data from Alpaca API for a given stock symbol.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): Stock symbol (e.g., \"SPY\")\n",
    "        timeframe (str): Timeframe for the bars (e.g., \"1Min\", \"5Min\", etc.)\n",
    "        start_date (str): Start date for the data (ISO 8601 format with 'T' and 'Z' time)\n",
    "        end_date (str): End date for the data (ISO 8601 format with 'T' and 'Z' time)\n",
    "        api_key (str): Alpaca API key\n",
    "        api_secret (str): Alpaca API secret\n",
    "        limit (int): Maximum number of bars to fetch (default is 10000)\n",
    "        adjustment (str): Data adjustment (default is 'raw')\n",
    "        feed (str): Data feed (default is 'sip')\n",
    "        sort (str): Sorting order ('asc' or 'desc', default is 'asc')\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response containing OHLC data\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://data.alpaca.markets/v2/stocks/{symbol}/bars?timeframe={timeframe}\"\n",
    "        f\"&start={start_date}&end={end_date}&limit={limit}&adjustment={adjustment}&feed={feed}&sort={sort}\"\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"APCA-API-KEY-ID\": api_key,\n",
    "        \"APCA-API-SECRET-KEY\": api_secret\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error fetching stock bars: {response.status_code}, {response.text}\")\n",
    "\n",
    "    # Print the response to check the structure\n",
    "    print(f\"Response JSON: {response.json()}\")\n",
    "\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_day_intraday_data(symbol, start_date, end_date, api_key, api_secret, max_calls_per_minute):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='B')  # Only business days\n",
    "    call_count = 0\n",
    "\n",
    "    for date in date_range:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        print(f\"Fetching data for {date_str}\")\n",
    "\n",
    "        # Modify this part to match the correct parameters expected by get_stock_bars\n",
    "        stock_bars_data = get_stock_bars(\n",
    "            symbol=symbol,\n",
    "            timeframe=\"1Min\",  # 1Min OHLC data\n",
    "            start_date=date_str,  # Adjust to match the expected format\n",
    "            end_date=date_str,  # Adjust to match the expected format\n",
    "            api_key=api_key,\n",
    "            api_secret=api_secret\n",
    "        )\n",
    "\n",
    "        # Check if there is data in the response\n",
    "        if stock_bars_data.get('bars') is None:\n",
    "            print(f\"No data found for {date_str}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Save data to CSV\n",
    "        save_underlying_ohlc_to_csv(stock_bars_data, date_str, symbol)\n",
    "\n",
    "        # Update call count and implement rate limiting\n",
    "        call_count += 1\n",
    "        if call_count >= max_calls_per_minute:\n",
    "            print(\"Rate limiting... waiting for 1 minute.\")\n",
    "            time.sleep(60)  # Pause for 60 seconds\n",
    "            call_count = 0  # Reset the call count after waiting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Option Contracts Based On Underlying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and generate unique contracts for each day based on underlying data\n",
    "def process_and_generate_all_contracts(df_underlying, strike_threshold=0, offset_days=2):\n",
    "    strike_ranges = calculate_strike_ranges(df_underlying, strike_threshold)\n",
    "    all_contracts = {}\n",
    "\n",
    "    for date_obj in sorted(df_underlying['date'].unique()):\n",
    "        date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "        if date_obj in strike_ranges:\n",
    "            low_strike, high_strike = strike_ranges[date_obj]\n",
    "            expiration_dates = get_expiration_dates(date_str, include_today=True, offset_days=offset_days)\n",
    "            contracts = generate_contracts_to_consider(df_underlying[df_underlying['date'] == date_obj], low_strike, high_strike, expiration_dates)\n",
    "            \n",
    "            # Ensure contracts are unique for each date\n",
    "            if date_str not in all_contracts:\n",
    "                all_contracts[date_str] = contracts\n",
    "            else:\n",
    "                all_contracts[date_str].extend(contracts)\n",
    "                all_contracts[date_str] = list(set(all_contracts[date_str]))  # Ensure uniqueness\n",
    "        else:\n",
    "            print(f\"No data available for {date_str}\")\n",
    "    \n",
    "    return all_contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Option Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_day_option_ohlc(contracts_by_date, api_key, api_secret, max_calls_per_minute=200):\n",
    "    \"\"\"\n",
    "    Fetch OHLC data for options contracts for each date, adhering to API call limits, and save into one file per day.\n",
    "\n",
    "    Parameters:\n",
    "        contracts_by_date (dict): A dictionary with dates as keys and lists of contracts as values.\n",
    "        api_key (str): Your Alpaca API key.\n",
    "        api_secret (str): Your Alpaca API secret.\n",
    "        max_calls_per_minute (int): Limit on the number of API calls per minute.\n",
    "    \"\"\"\n",
    "    call_count = 0\n",
    "    start_time = time.time()  # Track start time to enforce rate limit\n",
    "\n",
    "    for date_str, symbols in contracts_by_date.items():\n",
    "        print(f\"Fetching option data for {date_str}\")\n",
    "\n",
    "        consolidated_data = []  # Store all data for this day\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            print(f\"Fetching data for symbol {symbol} on {date_str}\")\n",
    "            \n",
    "            # Fetch 1-minute OHLC data for the full day using the fetch_option_bars function\n",
    "            option_bars_data = fetch_option_bars(\n",
    "                symbols=[symbol],  # Ensure symbol is passed as a list\n",
    "                timeframe=\"1Min\",  # 1Min OHLC data\n",
    "                # start=f\"{date_str}T09:30:00Z\",\n",
    "                # end=f\"{date_str}T16:00:00Z\",\n",
    "                start=f\"{date_str}\",\n",
    "                end=f\"{date_str}\",\n",
    "                api_key=api_key,\n",
    "                api_secret=api_secret\n",
    "            )\n",
    "\n",
    "            # Increment the API call counter\n",
    "            call_count += 1\n",
    "\n",
    "            # If there is no data, skip this symbol\n",
    "            if not option_bars_data or 'bars' not in option_bars_data:\n",
    "                print(f\"No data to save for {symbol} on {date_str}.\")\n",
    "                continue\n",
    "\n",
    "            # Convert the option bars data to a list of dictionaries\n",
    "            for symbol_key, bars in option_bars_data['bars'].items():\n",
    "                for bar in bars:\n",
    "                    bar['symbol'] = symbol_key\n",
    "                    consolidated_data.append(bar)\n",
    "\n",
    "            # Rate limiting: if we've hit the max calls per minute, pause\n",
    "            if call_count >= max_calls_per_minute:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                if elapsed_time < 60:  # If less than a minute has passed, wait\n",
    "                    sleep_time = 60 - elapsed_time\n",
    "                    print(f\"Rate limit reached. Sleeping for {sleep_time:.2f} seconds.\")\n",
    "                    time.sleep(sleep_time)\n",
    "                \n",
    "                # Reset the counter and start time after pausing\n",
    "                call_count = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "        # Convert the consolidated data into a DataFrame\n",
    "        if consolidated_data:\n",
    "            df = pd.DataFrame(consolidated_data)\n",
    "            # Convert timestamp to readable datetime format\n",
    "            df['t'] = pd.to_datetime(df['t'])\n",
    "\n",
    "            # Save the consolidated data for the day\n",
    "            save_options_ohlc_to_csv(df, date_str)\n",
    "\n",
    "        else:\n",
    "            print(f\"No data to save for {date_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_option_bars(symbols, timeframe=\"5Min\", start=None, end=None, limit=1000, sort=\"asc\", \n",
    "                      api_key=None, api_secret=None):\n",
    "    \"\"\"\n",
    "    Fetches option bars data for given option symbols from Alpaca Broker.\n",
    "\n",
    "    Parameters:\n",
    "        symbols (list or str): A list of option symbols or a single option symbol (e.g., [\"AAPL241220C00300000\", \"AAPL240315C00225000\"]).\n",
    "        timeframe (str): The timeframe for the bars (default is \"5Min\").\n",
    "        start (str): The start date in \"YYYY-MM-DD\" format.\n",
    "        end (str): The end date in \"YYYY-MM-DD\" format.\n",
    "        limit (int): The maximum number of bars to fetch (default is 1000).\n",
    "        sort (str): The sort order, either \"asc\" or \"desc\" (default is \"asc\").\n",
    "        api_key (str): Your Alpaca API key.\n",
    "        api_secret (str): Your Alpaca API secret.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the Alpaca API in JSON format.\n",
    "    \"\"\"\n",
    "    # Ensure symbols are properly formatted as a comma-separated string\n",
    "    if isinstance(symbols, list):\n",
    "        symbols = \",\".join(symbols)\n",
    "\n",
    "    url = f\"https://data.alpaca.markets/v1beta1/options/bars?symbols={symbols}&timeframe={timeframe}&limit={limit}&sort={sort}\"\n",
    "    \n",
    "    if start:\n",
    "        url += f\"&start={start}\"\n",
    "    if end:\n",
    "        url += f\"&end={end}\"\n",
    "\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"APCA-API-KEY-ID\": api_key,\n",
    "        \"APCA-API-SECRET-KEY\": api_secret\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the response is successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching option bars: {response.status_code}, {response.text}\")\n",
    "        return {}\n",
    "    \n",
    "    # Parse response JSON\n",
    "    response_data = response.json()\n",
    "\n",
    "    # Debugging: print the full response from the API\n",
    "    print(f\"API response for symbols {symbols}: {response_data}\")\n",
    "\n",
    "    # Check if there is any data in 'bars'\n",
    "    if 'bars' not in response_data or not response_data['bars']:\n",
    "        print(f\"No bars data returned for symbols: {symbols}\")\n",
    "        return {}\n",
    "\n",
    "    return response_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* merging\n",
    "* technical indicators\n",
    "* moneyness: ITM, ATM, OTM\n",
    "* greeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_underlying_price_and_moneyness(df_options, df_underlying):\n",
    "    # Ensure both DataFrames have compatible datetime formats and are tz-naive\n",
    "    df_options['t'] = pd.to_datetime(df_options['t']).dt.tz_localize(None)\n",
    "    df_underlying['t'] = pd.to_datetime(df_underlying['t']).dt.tz_localize(None)\n",
    "\n",
    "    # Print time ranges to check for overlap\n",
    "    print(f\"Options time range: {df_options['t'].min()} to {df_options['t'].max()}\")\n",
    "    print(f\"Underlying time range: {df_underlying['t'].min()} to {df_underlying['t'].max()}\")\n",
    "\n",
    "    # Trim the underlying data to ensure the time range aligns with options data\n",
    "    df_underlying_trimmed = df_underlying[df_underlying['t'] >= df_options['t'].min()]\n",
    "\n",
    "    # Print size of underlying after trimming\n",
    "    print(f\"Size of underlying after trimming: {df_underlying_trimmed.shape}\")\n",
    "\n",
    "    # Merge using the 'close' column from df_underlying, but ensure we are merging on the same minute-level timestamps\n",
    "    df = pd.merge_asof(\n",
    "        df_options.sort_values('t'),\n",
    "        df_underlying_trimmed[['t', 'close']].sort_values('t'),\n",
    "        on='t',\n",
    "        direction='backward',  # Find the last available price\n",
    "        tolerance=pd.Timedelta('1min')  # Match within 1-minute tolerance\n",
    "    )\n",
    "\n",
    "    # Print size of resulting DataFrame\n",
    "    print(f\"Size of merged DataFrame: {df.shape}\")\n",
    "    \n",
    "    # If the merged DataFrame is empty, return and stop further processing\n",
    "    if df.empty:\n",
    "        print(\"Warning: The merged DataFrame is empty!\")\n",
    "        return df\n",
    "\n",
    "    # Extract the last 6 digits from the symbol and divide by 1000 to get the strike price\n",
    "    df['strike_price'] = df['symbol'].str[-6:].astype(float) / 1000\n",
    "\n",
    "    # Define a function to label moneyness based on the option type and underlying close price\n",
    "    def label_moneyness(row):\n",
    "        if 'C' in row['symbol']:  # For Call options\n",
    "            if row['close'] > row['strike_price']:\n",
    "                return 'ITM'\n",
    "            elif row['close'] == row['strike_price']:\n",
    "                return 'ATM'\n",
    "            else:\n",
    "                return 'OTM'\n",
    "        elif 'P' in row['symbol']:  # For Put options\n",
    "            if row['close'] < row['strike_price']:\n",
    "                return 'ITM'\n",
    "            elif row['close'] == row['strike_price']:\n",
    "                return 'ATM'\n",
    "            else:\n",
    "                return 'OTM'\n",
    "        return None\n",
    "\n",
    "    # Apply moneyness labeling\n",
    "    df['moneyness'] = df.apply(label_moneyness, axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df):\n",
    "    df['RSI'] = ta.RSI(df['c'], timeperiod=14)\n",
    "    df['MACD'], df['MACD_signal'], df['MACD_hist'] = ta.MACD(df['c'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df['BB_upper'], df['BB_middle'], df['BB_lower'] = ta.BBANDS(df['c'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    df['SMA'] = ta.SMA(df['c'], timeperiod=20)\n",
    "    df['EMA'] = ta.EMA(df['c'], timeperiod=20)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_pipeline(df_options, df_underlying):\n",
    "    df_options = add_technical_indicators(df_options)\n",
    "    print(f\"After adding technical indicators: {df_options.shape}\")\n",
    "    \n",
    "    df_options = add_underlying_price_and_moneyness(df_options, df_underlying)\n",
    "    print(f\"After adding underlying price and moneyness: {df_options.shape}\")\n",
    "    \n",
    "#     df_options = add_greeks(df_options)\n",
    "#     print(f\"After adding Greeks: {df_options.shape}\")\n",
    "    \n",
    "    return df_options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actuals\n",
    "* Get underlying data\n",
    "* Get Option data\n",
    "* Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download underlying data\n",
    "fetch_full_day_intraday_data(\n",
    "    symbol=symbol,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    api_key=api_key,\n",
    "    api_secret=api_secret,\n",
    "    max_calls_per_minute=max_calls_per_minute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Determine contracts to download\n",
    "data = read_all_csvs(base_folder)\n",
    "df_underlying = data['underlying']\n",
    "all_contracts = process_and_generate_all_contracts(df_underlying, strike_threshold=strike_threshold, offset_days=days_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_underlying.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of all contracts\n",
    "\n",
    "def count_contracts_by_date(all_contracts):\n",
    "    \"\"\"\n",
    "    Returns the count of option contracts for each date.\n",
    "\n",
    "    Parameters:\n",
    "        all_contracts (dict): A dictionary with dates as keys and lists of contracts as values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with dates as keys and the count of contracts as values.\n",
    "    \"\"\"\n",
    "    contract_counts = {}\n",
    "    \n",
    "    for date, contracts in all_contracts.items():\n",
    "        contract_counts[date] = len(contracts)\n",
    "    \n",
    "    return contract_counts\n",
    "\n",
    "count_contracts_by_date(all_contracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download option data\n",
    "contracts_by_date = {date: contracts for date, contracts in all_contracts.items()}\n",
    "\n",
    "fetch_full_day_option_ohlc(\n",
    "    contracts_by_date=contracts_by_date,\n",
    "    api_key=api_key,\n",
    "    api_secret=api_secret,\n",
    "    max_calls_per_minute=max_calls_per_minute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_underlying.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_underlying.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Read saved data and preprocess\n",
    "data = read_all_csvs(base_folder)\n",
    "df_options_full_day = data['options_full_day']\n",
    "df_processed = preprocess_full_pipeline(df_options_full_day, df_underlying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Display processed data\n",
    "print(df_processed.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where you want to save the file\n",
    "\n",
    "save_path_underlying = r\"ENTER FILE PATH\"\n",
    "save_path_options = r\"ENTER FILE PATH\"\n",
    "\n",
    "# Save the DataFrame to the specified path as a CSV file\n",
    "df_underlying.to_csv(save_path_underlying, index=False)\n",
    "df_processed.to_csv(save_path_options, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {save_path_underlying}\")\n",
    "print(f\"DataFrame saved to {save_path_options}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
